{
  "cells": [
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Crossing the Freeway street with Proximal Policy Optimization and Random Network Distillation"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "This jupyter notebook contains the code used for the main routine of the implementation of the Atari game Freeway and the experiment on Venture. \n",
        "\n",
        "Additionally, in the directory, there are two additional modules:\n",
        "- **agent.py**: This file contains the code related to the agent, including taking actions in the environment, networks creation and optimization. \n",
        "- **utilities.py**: In this file there are 4 classes: \n",
        "  - EnvWrapper: a simple wrapper that manages the input dimensions. \n",
        "  - Normalizer: a wrapper that utilizes the Gym's class RunningStdMean to normalize the state and the reward in the RND settings.\n",
        "  - OptimizationBatch: used to collect all the data and create the minibatches during the optimization phase.\n",
        "  - InfoWriter: used to write the Tensorboard's logs. \n",
        "  \n",
        "All the graphs shown in the report are available in the directory \"./logs\", it's possible to visualize them using Tensorboard. Addiotionally, is possible to find the graphs of the entopy and those of the loss functions, that were used in the debugging phase."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "%load_ext tensorboard"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Note: code transparency\n",
        "To ensure completely transparency about the following code, here's a list of the material that inspired its implementation:\n",
        "- [CleanRL RND implementation](https://docs.cleanrl.dev/rl-algorithms/ppo-rnd/) \n",
        "  - This code was consulted to draw inspiration for implementing the RND algorithm. Specifically, the choice to use EnvPool and Gym's normalization class [RunningMeanStd](https://gymnasium.farama.org/main/_modules/gymnasium/wrappers/normalize/) (they followed the original RND code, where the authors used the same class as well) was influenced by this implementation. \n",
        "- [Machine Learning with Phil - Proximal Policy Optimization (PPO) | Full PPO Tutorial](https://www.youtube.com/watch?v=hlv79rcHws0)\n",
        "  - This helpful video was consulted to verify the correctness of the implemented code during the debugging phase.\n",
        "- Various other articles found online, but with any significant influence on this work.\n",
        "\n",
        "It is important to note that no lines of code were directly copied from these resources. They primarely served as a reference to provide a \"direction\" during the implementation and, in particular, in the debugging phase. "
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Settings and imports"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Is possible to load and learn every Atari game available in the EnvPool collection.\n",
        "The Freeway experiment with the RND bonus is performed using PPO parameters (rnd_hyperparams=False) and the exploration bonus activated (rnd_bonus=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "log                 = True\n",
        "seeds               = [42]\n",
        "name                = \"Freeway-v5\"\n",
        "log_info            = \"\"\n",
        "\n",
        "# Activate the exploration bonus \n",
        "rnd_bonus           = True\n",
        "\n",
        "# Select RND hyperparams or PPO's\n",
        "# This RND implementation only uses 32 parallel environments (instead of 128) and 8000 rollouts (instead of 30000) for hardware and time constraints.\n",
        "rnd_hyperparms      = False\n",
        "\n",
        "# RND settings: (~132 million frames => 8000 rollouts * 128 steps * 32 actors * 4 stacked frames)\n",
        "# PPO settings: (~40 million frames => 10000 rollouts * 128 steps * 8 actors * 4 stacked frames)\n",
        "tot_rlls            = 8000 if rnd_hyperparms else 10000\n",
        "\n",
        "# Is possible to set a threshold to go to the next seed\n",
        "thresh_next_seed    = 30000\n",
        "thresh_reward       = 5"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "#os.environ[\"TF_CPP_MIN_LOG_LEVEL\"] = \"2\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from tensorflow     import keras\n",
        "import envpool\n",
        "import numpy        as np\n",
        "import tensorflow   as tf\n",
        "from utilities      import *\n",
        "from tqdm.notebook  import tqdm \n",
        "from agent          import Agent\n",
        "from datetime       import datetime"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Hyperparameters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def set_seed(seed):\n",
        "    tf.keras.utils.set_random_seed(seed=seed)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "steps_rll       = 128    \n",
        "num_envs        = 32    if rnd_hyperparms else 8\n",
        "num_minib       = 4 \n",
        "steps_init      = num_envs * steps_rll\n",
        "steps_opt       = 4     if rnd_hyperparms else 3\n",
        "\n",
        "############################################\n",
        "ext_rwd_coeff   = 2 \n",
        "int_rwd_coeff   = 1 \n",
        "lr              = 1e-4  if rnd_hyperparms else 2.5e-4 \n",
        "optimizer       = keras.optimizers.legacy.Adam(learning_rate=lr)\n",
        "gae_lambda      = 0.95 \n",
        "entropy_coeff   = 0.001 if rnd_hyperparms else 0.01\n",
        "ext_rwd_gamma   = 0.999 if rnd_hyperparms else 0.99\n",
        "int_rwd_gamma   = 0.99\n",
        "clip_epsilon    = 0.1\n",
        "vf_coeff        = 1 \n",
        "\n",
        "\n",
        "############################################\n",
        "# Max episode frames is equal to 18000, every step 4 frame\n",
        "max_episode_steps = 18000 // 4 \n",
        "max_steps       = 10e6 // (steps_rll * num_envs)\n",
        "batch_size  = steps_rll * num_envs\n",
        "minib_size  = batch_size // num_minib\n",
        "\n",
        "############################################"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Environment settings\n",
        "\n",
        "Using EnvPool in order to manage multiple parallel envs\n",
        "https://envpool.readthedocs.io/en/latest/env/atari.html."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def get_env():\n",
        "    parallel_env = envpool.make(name,\n",
        "                        env_type    = \"gymnasium\",\n",
        "                        num_envs    = num_envs,\n",
        "                        seed        = 42,    # The default envpool seed\n",
        "                        frame_skip  = 4,\n",
        "                        img_height  = 84,\n",
        "                        img_width   = 84,\n",
        "                        stack_num   = 4,\n",
        "                        gray_scale  = True,\n",
        "                        reward_clip = True,\n",
        "                        max_episode_steps           = max_episode_steps if rnd_hyperparms else 27000,\n",
        "                        repeat_action_probability   = 0.25              if rnd_hyperparms else 0,\n",
        "                        )\n",
        "\n",
        "    return parallel_env"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Resources initialization"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Creating the agent, initializing the Optimization batch and the normalizers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def init_resources():\n",
        "        global running_int_returns, int_returns_continous\n",
        "        parallel_env = get_env()\n",
        "        env = EnvWrapper(parallel_env, name) \n",
        "\n",
        "        action_space_n  = env.action_n\n",
        "        state_shape     = env.observation_shape\n",
        "\n",
        "        policy_inshape  = (*state_shape, 1)\n",
        "        policy_outshape = action_space_n\n",
        "        \n",
        "        rnd_inshape     = (1, *policy_inshape[1:]) \n",
        "        rnd_outshape    = policy_outshape\n",
        "\n",
        "        player = Agent(\n",
        "                env             = env,\n",
        "                n_envs          = num_envs,\n",
        "                action_space_n  = action_space_n,\n",
        "                int_rwd_gamma   = int_rwd_gamma,\n",
        "                ext_rwd_gamma   = ext_rwd_gamma,\n",
        "                gae_lambda      = gae_lambda,\n",
        "                batch_size      = batch_size,\n",
        "                minib_s         = minib_size,\n",
        "                rnd_bonus       = rnd_bonus,\n",
        "                policy_inshape  = policy_inshape,\n",
        "                policy_outshape = policy_outshape,\n",
        "                rnd_inshape     = rnd_inshape,\n",
        "                rnd_outshape    = rnd_outshape,\n",
        "                clip_epsilon    = clip_epsilon,\n",
        "                entropy_coeff   = entropy_coeff,\n",
        "                vf_coeff        = vf_coeff,\n",
        "                optimizer       = optimizer,\n",
        "                verbose         = False\n",
        "                )\n",
        "\n",
        "        batch = OptimizationBatch(steps_rll, num_envs, policy_inshape)\n",
        "        \n",
        "        \n",
        "        running_int_returns = np.zeros((steps_rll, num_envs))\n",
        "        int_returns_continous = np.zeros((num_envs))\n",
        "\n",
        "        norm_obs = Normalizer(\"state\", rnd_inshape)\n",
        "        norm_rwd = Normalizer(\"rwd\", (num_envs))\n",
        "\n",
        "        return player, batch, norm_obs, norm_rwd"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "if log:\n",
        "    date_str    = datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
        "    log_dir     = \"logs/scalars/\" + name + \"/\" + date_str + f\"_{num_envs}\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "seed = seeds.pop()\n",
        "set_seed(seed)\n",
        "player, batch, norm_obs, norm_rwd = init_resources()\n",
        "if log:\n",
        "    info_writer = InfoWriter(log_dir, num_envs)\n",
        "    info_writer.log_seed(seed, log_info) "
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Play for timesteps "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def play_for_steps(state, tot_steps, random = False):\n",
        "\n",
        "    ended_this_play = 0\n",
        "\n",
        "    for _ in range(1, tot_steps + 1):\n",
        "        next_state, ext_rwd, done, action, action_logit, ext_value, int_value = player.play_one_step(state, random=random)\n",
        "        \n",
        "        if not random:\n",
        "            if rnd_bonus:\n",
        "                norm_state  = norm_obs(next_state)\n",
        "                int_rwd     = player.compute_int_rwd(norm_state)\n",
        "            else: int_rwd = np.zeros_like(ext_rwd)\n",
        "            \n",
        "            ended = np.nonzero(done)[0]\n",
        "            if np.size(ended) > 0:\n",
        "                next_state[ended] = player.env.reset_ended(ended)      \n",
        "            \n",
        "            # Add experience to the buffer queue\n",
        "            batch.append(state, action, action_logit, ext_rwd, int_rwd, ext_value, int_value, done)\n",
        "\n",
        "            if log:\n",
        "                info_writer.update_ext_rewards(ext_rwd, int_rwd)\n",
        "                for idx in ended:\n",
        "                    ended_this_play += 1\n",
        "                    info_writer.update_ended(idx, ended_this_play)\n",
        "\n",
        "        else: \n",
        "            # Update the norms normalizer in the random steps\n",
        "            norm_obs.update(state)\n",
        "\n",
        "        state = next_state\n",
        "\n",
        "    if log and ended_this_play > 0:\n",
        "        info_writer.update_last_reward()                  \n",
        "    return state"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Intrinsic reward normalization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def discount_int_rwds(int_rwds):\n",
        "    global running_int_returns, int_returns_continous\n",
        "\n",
        "    for step, rwd in reversed(list(enumerate(int_rwds))):\n",
        "        running_int_returns[step] = rwd + int_rwd_gamma * int_returns_continous\n",
        "        int_returns_continous = running_int_returns[step]\n",
        "\n",
        "    return running_int_returns\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def normalize_int_rwds(int_rwds):\n",
        "    #Normalize the intrinsic reward with running estimate int returns, continuosly\n",
        "    updated_int_returns = discount_int_rwds(int_rwds)\n",
        "    \n",
        "    # Update norm rwd params\n",
        "    norm_rwd.update_reward(updated_int_returns)\n",
        "\n",
        "    # Normalize int rwds\n",
        "    norm_int_rwds = norm_rwd(int_rwds)\n",
        "    return norm_int_rwds"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Train routine"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def train_networks(step):\n",
        "    info_writer.reset_train_infos()\n",
        "\n",
        "    for opt in range(1, steps_opt + 1):\n",
        "        # Optimize theta_pi wrt PPO loss on batch, R and A using Adam\n",
        "        sums_info = player.training_step_ppo(num_minib, batch)\n",
        "        \n",
        "        if rnd_bonus:\n",
        "            # Optimize theta_f^ wrt distillation loss on batch using Adam        \n",
        "            rnd_train_states    = norm_obs(batch.states)\n",
        "            rnd_loss            = player.training_step_distill(num_minib, rnd_train_states)\n",
        "        else: rnd_loss = 0\n",
        "        \n",
        "        sums_info = sums_info + (rnd_loss,)\n",
        "\n",
        "        if log:\n",
        "            info_writer.incremental_mean_train(opt, sums_info)\n",
        "            \n",
        "    if log:\n",
        "        info_writer.write_infos(step)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Main routine"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The scaling of the clip epsilon and the learning rate is only active with the PPO settings"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def scale_alpha(step):\n",
        "    alpha = ((max_steps - step)/max_steps)\n",
        "    player.clip_epsilon     = clip_epsilon * alpha\n",
        "    player.optimizer.learning_rate = lr * alpha"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def check_end_conditions(step):\n",
        "    reward_mean = info_writer.get(\"ext_rwd\")\n",
        "    if (step % 500 == 0):\n",
        "        print(\"Saving checkpoints...\")\n",
        "        player.save_checkpoints(date_str)\n",
        "        \n",
        "    if step >= thresh_next_seed and reward_mean <= thresh_reward:\n",
        "        print(\"Moving to the next seed\")\n",
        "        return True\n",
        "\n",
        "    return False"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def ppo_rnd_algorithm():\n",
        "    \n",
        "    state = player.reset_env()\n",
        "    if rnd_bonus:\n",
        "        # Initialize normalization parameters playing randomly\n",
        "        state = play_for_steps(state, steps_init, random=True)\n",
        "        \n",
        "    for step in tqdm(range(1, tot_rlls + 1)):\n",
        "        \n",
        "        # Reset the batch\n",
        "        batch.reset()\n",
        "\n",
        "        # Play steps_rll steps and collect the data in the batch\n",
        "        next_state = play_for_steps(state, steps_rll)\n",
        "        _ , next_ext_value, next_int_value = player._forward_policy(next_state)\n",
        "        state = next_state\n",
        "\n",
        "        # Compute returns and advantages for extrinsic and intrisic rewards\n",
        "        advs_ext, returns_ext = player.calculate_advs_and_returns(\"ext\", batch.ext_rwds, batch.ext_values, next_ext_value, batch.dones)\n",
        "        \n",
        "        if rnd_bonus:\n",
        "            batch.int_rwds = normalize_int_rwds(batch.int_rwds)\n",
        "            info_writer.update_int_rewards(batch.int_rwds)\n",
        "            advs_int, returns_int = player.calculate_advs_and_returns(\"int\", batch.int_rwds, batch.int_values, next_int_value, np.zeros(batch.values_shape))       \n",
        "            # Combine advs and rewards\n",
        "            advs_combined = ext_rwd_coeff * advs_ext + int_rwd_coeff * advs_int\n",
        "        else: \n",
        "            returns_int = np.zeros_like(returns_ext)\n",
        "            advs_combined = advs_ext\n",
        "\n",
        "        if log:\n",
        "            info_writer.update_advs_returns(advs_combined, returns_ext, returns_int)\n",
        "\n",
        "        # Add the calculated advantages and returns to the batch and resize the batch \n",
        "        batch.add_advs_returns(advs_combined, returns_ext, returns_int)\n",
        "        batch.resize_for_minibatch()\n",
        "\n",
        "        if not rnd_hyperparms:\n",
        "            scale_alpha(step)\n",
        "\n",
        "        if rnd_bonus:\n",
        "            # Update obs normalization using the batch\n",
        "            norm_obs.update(batch.states)\n",
        "        \n",
        "        # Perform the training steps\n",
        "        train_networks(step)\n",
        "\n",
        "        if check_end_conditions(step):\n",
        "            break   "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Can be manually used to restore a checkpoint\n",
        "\n",
        "def restore_session():\n",
        "    date = \"2023_06_23\"\n",
        "    player.restore_checkpoints(date_str = date)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(f\"##################### Starting main routine #####################\")\n",
        "print(f\"Total steps:\\t\\t{tot_rlls} \")\n",
        "print(f\"Logging:\\t\\t{log}\")\n",
        "print(f\"RND bonus:\\t\\t{rnd_bonus}\")\n",
        "if rnd_hyperparms:\n",
        "    print(f\"RND hyperparameters\")\n",
        "else: \n",
        "    print(f\"PPO hyperparameters\")\n",
        "\n",
        "# restore_session()\n",
        "\n",
        "while True:\n",
        "    print(f\"Seed:\\t\\t\\t{seed}\")\n",
        "    ppo_rnd_algorithm()\n",
        "    if log:\n",
        "        print(\"Saving the model\")\n",
        "        path = f\"./saved_model/{name}/\" + date_str + f\"_{seed}\"\n",
        "        player.policy.save(path + \"/policy\")\n",
        "    if not seeds:\n",
        "        break\n",
        "    else:\n",
        "        seed = seeds.pop()\n",
        "        set_seed(seed)\n",
        "        player, batch, norm_obs, norm_rwd = init_resources()\n",
        "        if log:\n",
        "            info_writer = InfoWriter(log_dir, num_envs)\n",
        "            info_writer.log_seed(seed, log_info) "
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Too see real time graphs:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "%load_ext tensorboard"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.8"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
